---
title: "Regression Models for Cats"
output: 
  github_document:
    toc: true
    toc_depth: 3
  html_document:
    keep_md: false
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.path = "md-images/cats-reg-"
)
```


# Setup

## Load Packages

```{r}
library(tidyverse)

library(ggplot2)
library(forecast)

theme_set(theme_bw())
```

## Load Data & Create Time Series for Cats

* Original data ranged from Oct 2013 to Aug 2020
* Subset data to drop months after Feb 2020 (impacted by COVID-19 lockdowns).  This leaves 77 months (or 6 years and 5 months) of animal intake from Oct 2013 to Feb 2020 
* Create time-series object with data for only the intake of cats

```{r}
DATA_ORIGINAL <- read.csv("data/Austin_AC_Monthly_Strays.csv", stringsAsFactors = FALSE)
# str(DATA_ORIGINAL)

DATA_SUBSET <- subset(DATA_ORIGINAL, IN_YRMO <= "2020-02")

TIME_SERIES <- ts(data      = DATA_SUBSET[,c("Cat")],    # only include Cats 
                  start     = c(2013, 10), 
                  end       = c(2020,  2), 
                  frequency = 12)
```

Plot the intake of stray cats

```{r}
label_title = "Cat Stray Intake Oct 2013 - Feb 2020"
label_xaxis = "Months"
label_yaxis = "Monthly Cat Intake"

autoplot(TIME_SERIES) +
  ggtitle(label_title) +
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title=""))
```

## Partition Data

* Training - 65 months or 5 years and 5 months (Oct 2013 to Feb 2019)
* Test - last 12 months (Mar 2019 to Feb 2020)

```{r}
TS_TRNG_SET <- window(TIME_SERIES, 
                      start=c(2013, 10), 
                      end  =c(2019,  2))

TS_TEST_SET <- window(TIME_SERIES,
                      start=c(2019,  3),
                      end  =c(2020,  2))

N_TEST_SET <- length(TS_TEST_SET)

TS_TEST_SET
```

# Benchmark - Seasonal Naive Forecast

A seasonal naive forecast projects the last full season of the training data forward as the forecast.  

```{r}
FC_SEASONAL_NAIVE <- snaive(TS_TRNG_SET, h=N_TEST_SET)

# snaive() returns an object of class forecast.  Element "mean" is the point forecast
FC_SEASONAL_NAIVE$mean
```

To confirm ... display last full season (12 months) of training data

```{r}
subset(TS_TRNG_SET, start=length(TS_TRNG_SET)-(N_TEST_SET-1))
```

Plot

* the full time series of cat intake (includes both the training and test data)
* the fitted values (basically shifts time series forward 1 season (=year in this case))
* the forecast for the next 12 months

```{r}
autoplot(TIME_SERIES, color="darkgray") +
#  ggtitle(label_title) +
  ggtitle("Cat: Seasonal Naive") +  
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +
  
  # fitted values
  autolayer(fitted(FC_SEASONAL_NAIVE), series="Seasonal Naive", lwd=0.5) +
  
  # forecasted values
  autolayer(FC_SEASONAL_NAIVE,         series="Seasonal Naive", PI=FALSE, lwd=1, lty=1)
```

**Residual Diagnostics** (from https://otexts.com/fpp2/residuals.html)

The “residuals” in a time series model are what is left over after fitting a model.

Residuals are useful in checking whether a model has adequately captured the information in the data. A good forecasting method will yield residuals with the following properties:

1. The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
2. The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

Checking these properties is important in order to see whether a method is using all of the available information, but it is not a good way to select a forecasting method.

In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.

3. The residuals have constant variance.
4. The residuals are normally distributed.

All of these methods for checking residuals are conveniently packaged into one R function `checkresiduals()`, which will produce a time plot, ACF plot and histogram of the residuals (with an overlaid normal distribution for comparison), and do a Ljung-Box test with the correct degrees of freedom.

```{r}
checkresiduals(FC_SEASONAL_NAIVE)
```


```{r}
summary(residuals(FC_SEASONAL_NAIVE))
```

**Residual Diagnostics on Seasonal Naive**

1. Residuals are correlated.  Spikes beyond significance line at lag 1, 12, 13.  Significant p-value from Ljung-Box test (residuals are distinguishable from a white noise series)
2. Mean is close to zero (-7.7).  From histogram generally normal distribution but couple of outliers (-400, +300) and peak just above 0 (slight positive bias)

Conclusion:  There is more information in time series than seasonal naive is extracting for a forecast.  Not surprising for a simple benchmark.

**Assess Accuracy of Model**

`forecast::accuracy()` calculates various metrics on both training and test data sets.  Interested in "Test set" results

```{r}
accuracy(FC_SEASONAL_NAIVE, TS_TEST_SET)
```

Using RMSE, on average forecast off by 132 cats per month (or 21% MAPE)

Manually calculate desired metrics

```{r}
ACTUAL    <- as.numeric(TS_TEST_SET)
PREDICTED <- as.numeric(FC_SEASONAL_NAIVE$mean)

MODEL_ACCURACY <- data.frame(
  Model    = "Seasonal Naive",
  RMSE     = Metrics::rmse(ACTUAL, PREDICTED),
  MAE      = Metrics::mae( ACTUAL, PREDICTED),
  MAPE     = Metrics::mape(ACTUAL, PREDICTED),
  AdjRsqrd = NA
)

MODEL_COMPARISON <- MODEL_ACCURACY
MODEL_COMPARISON
```

# Regression Models

## Model Seasonality

Just have seasonality (months) as predictors.

**Fit Model**

```{r}
MODEL_SEASONALITY <- tslm(formula = TS_TRNG_SET ~ season,
                          data    = TS_TRNG_SET)

summary(MODEL_SEASONALITY)

```

Adjusted R-squared:  0.8863 - model explains 88.6% of variance in data!

**Forecast/Predict Values for Test Set**

```{r}
FC_SEASONALITY <- forecast(MODEL_SEASONALITY, h=N_TEST_SET)
FC_SEASONALITY$mean
```

Plot

* the full time series of cat intake (includes both the training and test data)
* the fitted values
* the forecast for the next 12 months

```{r}
autoplot(TIME_SERIES, color="darkgray") +
#  ggtitle(label_title) +
  ggtitle("Cat: Regression on Seasonality") + 
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +

  # fitted values  
  autolayer(fitted(MODEL_SEASONALITY), series=" Seasonality", lwd=0.5) +

  # forecasted values
  autolayer(FC_SEASONALITY,            series=" Seasonality", PI=FALSE, lwd=1, lty=1)
```

For comparison, add Seasonal Naive forecast to plot

```{r}
autoplot(TIME_SERIES, color="darkgray") +
#  ggtitle(label_title) +
  ggtitle("Cat: Regression on Seasonality vs. Seasonal Naive") +
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +

  autolayer(fitted(MODEL_SEASONALITY), series="Regression on Seasonality", lwd=0.5) +
  autolayer(FC_SEASONALITY,            series="Regression on Seasonality", PI=FALSE, lwd=1, lty=1) +

  autolayer(FC_SEASONAL_NAIVE,         series="Seasonal Naive", PI=FALSE, lwd=.75, lty=1)

# ggsave(filename = "figures/02_MOD_Cat_SeasonReg_vs_Naive.png",
#        height=4, width=6, units="in")
```

```{r}
summary(residuals(MODEL_SEASONALITY))

checkresiduals(MODEL_SEASONALITY)
```

> Another useful test of autocorrelation in the residuals designed to take account for the regression model is the Breusch-Godfrey test, also referred to as the LM (Lagrange Multiplier) test for serial correlation. It is used to test the joint hypothesis that there is no autocorrelation in the residuals up to a certain specified order. **A small p-value indicates there is significant autocorrelation remaining in the residuals.**
>
>The Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically designed for use with regression models.

**Residual Diagnostics on Regression with Seasonality**

1. Residuals are correlated.  Spikes beyond significance line at lag 1, 4.  Significant p-value (0.02 at an alpha of 0.05) from Breusch-Godfrey test (residuals are distinguishable from a white noise series)
2. Mean is zero.  From histogram generally normal distribution but couple of outliers (-175, +230) and peak just below 0 (slight negative bias)

Conclusion: Better than seasonal naive but there is more information in time series the model is not extracting for a forecast.

**Assess Accuracy of Model**

```{r}
accuracy(FC_SEASONALITY, TS_TEST_SET)
```

```{r}
ACTUAL    <- as.numeric(TS_TEST_SET)
PREDICTED <- as.numeric(FC_SEASONALITY$mean)

MODEL_ACCURACY <- data.frame(
  Model    = "Seasonality",
  RMSE     = Metrics::rmse(ACTUAL, PREDICTED),
  MAE      = Metrics::mae( ACTUAL, PREDICTED),
  MAPE     = Metrics::mape(ACTUAL, PREDICTED),
  AdjRsqrd = 0.8863
)

MODEL_COMPARISON <- rbind(MODEL_COMPARISON, MODEL_ACCURACY)
MODEL_COMPARISON
```

Improvement over Seasonal Naive

## Model Linear Trend

**Fit Model**

```{r}
MODEL_SEASON_LIN_TREND <- tslm(formula = TS_TRNG_SET ~ season + trend,
                               data    = TS_TRNG_SET)

summary(MODEL_SEASON_LIN_TREND)
```

Adjusted R-squared:  0.8947 (vs 0.8863) - model explains 89.4%.  A slight improvement.

**Forecast/Predict Values for Test Set**

```{r}
FC_SEASON_LIN_TREND <- forecast(MODEL_SEASON_LIN_TREND, h=N_TEST_SET)
FC_SEASON_LIN_TREND$mean
```

```{r}
autoplot(TIME_SERIES, color="darkgray") +
#  ggtitle(label_title) +
  ggtitle("Cat: Regression on Seasonality+Linear Trend") +  
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +

  # fitted values      
  autolayer(fitted(MODEL_SEASON_LIN_TREND), series="Season+Linear Trend", lwd=0.5) +
    
  # forecasted values    
  autolayer(FC_SEASON_LIN_TREND,            series="Season+Linear Trend", PI=FALSE, lwd=1, lty=1)
```

For comparison, plot regression on just seasonality

```{r}
autoplot(TIME_SERIES, color="darkgray") +
#  ggtitle(label_title) +
  ggtitle("Cat: Regression on Seasonality+Linear Trend vs. Seasonality") +  
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +

  # fitted values      
  autolayer(fitted(MODEL_SEASON_LIN_TREND), series="Season+Linear Trend", lwd=0.5) +
    
  # forecasted values    
  autolayer(FC_SEASON_LIN_TREND,            series="Season+Linear Trend", PI=FALSE, lwd=1, lty=1) +

  # compare with forecast on just seasonality
  autolayer(FC_SEASONALITY,                 series="Seasonality",         PI=FALSE, lwd=.75, lty=1)
```

There's a tiny negative trend coefficient which would forecast/predict (1 fewer cats) for each future month than its prior month. And the coefficient for each month are a bit lower as well.  That shifts/pulls the forecast down slightly, but in the test data the actual intake went up.  So the error of the forecast will increase by including a trend term in the model.

```{r}
summary(residuals(MODEL_SEASON_LIN_TREND))

checkresiduals(MODEL_SEASON_LIN_TREND)
```

**Residual Diagnostics on Regression with Seasonality+Linear Trend**

1. Residuals correlated??  Spikes beyond significance line at lag 1, 12.  p-value is not significant (0.085 at an alpha of 0.05) from Breusch-Godfrey test (indicates residuals are *NOT* distinguishable from a white noise series)
2. Mean is zero.  From histogram generally normal distribution but couple of outliers (-175, +220) and peak just below 0 (slight negative bias)

Conclusion: Seems to fit the training data a litte better than model with just seasonality.

```{r}
accuracy(FC_SEASON_LIN_TREND, TS_TEST_SET)
```

```{r}
ACTUAL    <- as.numeric(TS_TEST_SET)
PREDICTED <- as.numeric(FC_SEASON_LIN_TREND$mean)

MODEL_ACCURACY <- data.frame(
  Model = "Season+Linear Trend",
  RMSE     = Metrics::rmse(ACTUAL, PREDICTED),
  MAE      = Metrics::mae( ACTUAL, PREDICTED),
  MAPE     = Metrics::mape(ACTUAL, PREDICTED),
  AdjRsqrd = 0.8947
)

MODEL_COMPARISON <- rbind(MODEL_COMPARISON, MODEL_ACCURACY)
MODEL_COMPARISON
```

Higher error as expected on test data given the forecast is shift down a bit over the just seasonality model.

## Model Polynomial

The trend-cycle of cat intake had a slight 3rd order polynomial look to it.

```{r}
MODEL_SEASON_POLY_TREND <- tslm(formula = TS_TRNG_SET ~ season + trend + I(trend^2) + I(trend^3),
                                data    = TS_TRNG_SET)

summary(MODEL_SEASON_POLY_TREND)
```

Adjusted R-squared: 0.9102 better (vs .895, .886)

```{r}
FC_SEASON_POLY_TREND <- forecast(MODEL_SEASON_POLY_TREND, h=N_TEST_SET)
FC_SEASON_POLY_TREND$mean
```

```{r}
autoplot(TIME_SERIES, color="darkgray") +
#  ggtitle(label_title) +
  ggtitle("Cat: Regression on Seasonality+Polynomial Trend") +  
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +

  autolayer(fitted(MODEL_SEASON_POLY_TREND), series="Season+Poly Trend", lwd=0.5) +
  autolayer(FC_SEASON_POLY_TREND,            series="Season+Poly Trend", PI=FALSE, lwd=1, lty=1) 
```

```{r}
checkresiduals(MODEL_SEASON_POLY_TREND)
```

```{r}
accuracy(FC_SEASON_POLY_TREND, TS_TEST_SET)
```

```{r}
ACTUAL    <- as.numeric(TS_TEST_SET)
PREDICTED <- as.numeric(FC_SEASON_POLY_TREND$mean)

MODEL_ACCURACY <- data.frame(
  Model    = "Season+Poly Trend",
  RMSE     = Metrics::rmse(ACTUAL, PREDICTED),
  MAE      = Metrics::mae( ACTUAL, PREDICTED),
  MAPE     = Metrics::mape(ACTUAL, PREDICTED),
  AdjRsqrd = 0.9102
)

MODEL_COMPARISON <- rbind(MODEL_COMPARISON, MODEL_ACCURACY)
MODEL_COMPARISON
```

All the accuracy metrics are bit worse than the Seasonality model - likely overfitting the training data.


## Model Seasonality + ARIMA Error Correction

I also attempted to use ARIMA as an error correction technique.  

There were some autocorrelations in the residuals of the seasonal regression model (e.g., 0.45 at lag 1).

```{r}
#checkresiduals(MODEL_SEASONALITY)

STG_1_RESIDUALS <- residuals(MODEL_SEASONALITY)

ggAcf(STG_1_RESIDUALS)
```

```{r}
ACF <- Acf(STG_1_RESIDUALS) 
ACF[1:12]
```

Create an ARIMA model on residuals


```{r}
MODEL_STG2_ARIMA <- auto.arima(STG_1_RESIDUALS, seasonal=FALSE)
#MODEL_STG2_ARIMA <- Arima(STG_1_RESIDUALS, order = c(1,0,0))

summary(MODEL_STG2_ARIMA)
```

I allowed the forecast’s package auto.arima() function to select the best ARIMA order which resulted in an ARIMA(1,1,1) model.  This is, the residuals are first differenced to make the residual time series stationary, then it factors in the value at the prior point in time (the AR(1) component) and a prior error component (MA(1)).  


The forecast for the residual correction started at about -6 and ended about -21.  

```{r}
FC_STG2_ARIMA <- forecast(MODEL_STG2_ARIMA, h=N_TEST_SET)
round(FC_STG2_ARIMA$mean,3)
```

```{r}
label_series <- "ARIMA(1,1,1)"
autoplot(STG_1_RESIDUALS, color="darkgray") +
  ggtitle("Stage 1 Residuals") +
  xlab("Months") + ylab("Residuals") +

  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +
  
  autolayer(fitted(FC_STG2_ARIMA),    series=label_series, lwd=.5) +
  autolayer(FC_STG2_ARIMA,            series=label_series, lwd=.75, lty=2, PI=FALSE) 
```

Combining the ARIMA error correction forecast end up lowering the original seasonal regression forecast.

Looking at the regression forecast, it is already under forecasting the actual values, so lowering the forecasted cat intake only increase the error (RMSE) and did not help in this case.

```{r}
FC_SEASONALITY_ARIMA <- FC_SEASONALITY$mean + FC_STG2_ARIMA$mean

autoplot(TIME_SERIES, color="darkgray") +
#  ggtitle(label_title) +
  ggtitle("Cat: Regression on Seasonality+ARIMA Error Correction") +  
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +
  
  #autolayer(fitted(MODEL_SEASONALITY), series="Seasonality", lwd=1) +
  autolayer(FC_SEASONALITY_ARIMA,      series="Seasonality+ARIMA",     lwd=1,   lty=1) +
  autolayer(FC_SEASONALITY,            series="Seasonality", PI=FALSE, lwd=.5,  lty=2) 
```

```{r}
ACTUAL    <- as.numeric(TS_TEST_SET)
PREDICTED <- as.numeric(FC_SEASONALITY_ARIMA)

MODEL_ACCURACY <- data.frame(
  Model    = "Seasonality+ARIMA",
  RMSE     = Metrics::rmse(ACTUAL, PREDICTED),
  MAE      = Metrics::mae( ACTUAL, PREDICTED),
  MAPE     = Metrics::mape(ACTUAL, PREDICTED),
  AdjRsqrd = NA
)

#MODEL_ACCURACY

MODEL_COMPARISON <- rbind(MODEL_COMPARISON, MODEL_ACCURACY)
MODEL_COMPARISON
```

## ARIMA

In class, we did not cover ARIMA models in depth.  It is a substantial topic in its own right.

Here, I'm am just throwing in example of walking through the steps.

```{r}
MODEL_ARIMA <- auto.arima(TS_TRNG_SET)

summary(MODEL_ARIMA)
```


```{r}
FC_ARIMA <- forecast(MODEL_ARIMA, h=N_TEST_SET)
#FC_ARIMA
```

```{r}
label_series <- "ARIMA(1,0,0)(1,1,0)[12]"

autoplot(TIME_SERIES, color="darkgray") +
  ggtitle(label_title) +
  xlab(label_xaxis) + ylab(label_yaxis) +
  theme(legend.position = "bottom") +
  guides(colour=guide_legend(title="")) +

  autolayer(fitted(MODEL_ARIMA), series=label_series, lwd=.75) +
  autolayer(FC_ARIMA,            series=label_series, PI=FALSE, lwd=1, lty=1) +
  
  #autolayer(fitted(MODEL_SEASONALITY), series="Seasonality", lwd=1) +
  autolayer(FC_SEASONALITY,            series="Seasonality", PI=FALSE, lwd=.75, lty=1) 
```

```{r}
checkresiduals(MODEL_ARIMA)
```

```{r}
ACTUAL    <- as.numeric(TS_TEST_SET)
PREDICTED <- as.numeric(FC_ARIMA$mean)

MODEL_ACCURACY <- data.frame(
  Model    = "ARIMA",
  RMSE     = Metrics::rmse(ACTUAL, PREDICTED),
  MAE      = Metrics::mae( ACTUAL, PREDICTED),
  MAPE     = Metrics::mape(ACTUAL, PREDICTED),
  AdjRsqrd = NA
)

MODEL_COMPARISON <- rbind(MODEL_COMPARISON, MODEL_ACCURACY)
MODEL_COMPARISON
```

```{r}
#write_csv(MODEL_COMPARISON, "data/results_cat_regresssion.csv")
```

