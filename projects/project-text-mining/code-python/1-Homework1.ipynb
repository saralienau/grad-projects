{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIA 6304 Text Mining, Fall 2019 - Assignment 1\n",
    "### by Sara Lienau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*My notebook is rather verbose. However, I organized it with headers and a table of content to make it easier to navigate and zero in on the material of interest to you. The notes are mainly for me to keep track of what I did and why and what I took away from trying different things.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Question 1 - Research Question](#Question-1---Research-Question)\n",
    "- [Question 2 - Creating a Bag of Words](#Question-2---Creating-a-Bag-of-Words)\n",
    "- [Question 3 - Results and Next Steps](#Question-3---Results-and-Next-Steps)\n",
    "\n",
    "- [Task 1 - Create DataFrame of Text Scrapped from Website](#Task-1A---Create-a-Data-Frame-Containing-Text-from-a-Website)\n",
    "- [Task 2 - Experiment with Vectorizers](#Task-2---Apply-a-Count-Vectorizer-&-Create-a-Bag-of-Words)\n",
    "  - [Variation 1 - Default - Feature Exists](#Variation-1---Default---Feature-Exists)\n",
    "    - [Variation 1.A - Default - Feature Frequency](#Variation-1-A---Default---Feature-Frequency)\n",
    "  - [Variation 2 - Preserve Original Case](#Variation-2---Preserve-Original-Case)\n",
    "  - [Variation 3 - Adjust RegEx to Preserve Hyphenated Words](#Variation-3---Adjust-RegEx-to-Preserve-Hyphenated-Words)\n",
    "  - [Variation 4 - Exclude Stop Words](#Variation-4---Exclude-Stop-Words)\n",
    "  - [Variation 5 - Prevalent Terms](#Variation-5---Prevalent-Terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Research Question\n",
    "*Note:  I am a full-time student. But I will imagine I am back at my previous job. My motivation for the project is to understand how different industries and companies are using AI (or data analytics). It will help me determine where I might want to pursue employment after I complete the MS-BIA degree.*\n",
    " \n",
    "My boss and I are curious about how AI is being used at other companies. I will use the [WSJ Pro Artificial Intelligence](https://www.wsj.com/pro/artificial-intelligence/newsletters) daily newsletter to do a literature review. I want to use the articles to look at the applications of AI, its business value, company/industry, and whether the outcome was favorable or not. The WSJ is not the be all end all, but one viewpoint and the newsletter’s focus is a good starting point. (The newsletter targets “those who want to better understand how artificial intelligence is transforming the business landscape, and more importantly how to apply AI to strategic and operational decisions.”)\n",
    " \n",
    "Some additional data I need is a way to map companies mentioned in the articles to their respective industry. I am not sure the precision by which I will be able to assess the business value of the application of AI. I will probably need a way to estimate something like “t-shirt size” (i.e., small, medium, large, extra-large). I will also have to work through a taxonomy of the uses of AI. I need to dig into the content of the articles to figure that out. The newsletter has titles for some articles and section headings for topics (e.g., robotics, facial recognition, operations, health care) so that is a starting point for categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Creating a Bag of Words\n",
    "My corpus is a collection of titles of articles in the WSJ Pro AI Newsletter from Jan to Aug 2019 (about 8 months).  It contains 474 documents (the title of an article). In Task 2, I experimented with various methods to tokenize the documents into words or phrases.  And then identify the existence or count the frequency of the tokens.  This process of turning text into a number is called vectorization.\n",
    " \n",
    "My documents are very short.  An article title typically contains 3 to 6 words with a maximum of 10.  As you might expect, there are not many words repeated within a title, so I focused on the existence of a token in the document rather than counting its frequency.  I first concentrated on whether the resulting tokens are a good representation of the terms in the article titles.  Through various adjustments, I bounced between 1000-1100 features.  And at the end, I zeroed in on the most popular terms (1 or 2-word phrases contained in at least 1% of the documents or five titles), and that reduced the feature space dramatically to 42 terms (versus 1000).  \n",
    " \n",
    "Summary of Variations:\n",
    "- Most of the words of the titles are capitalized, so converting text to lower case did not reduce the feature space much.  (There were not many variations of capitalized and lower-case words to consolidate.) \n",
    "- I adjusted the token regular expression to include hyphenated words and abbreviations.  Without this change, the tokenization process was either ignoring useful features such as U.S. or U.K. or decomposing one logical word into multiple features (e.g., Ex-Google into ex and google or Open-Source as open and source).  The hyphenated term is more meaningful.\n",
    "- I used a custom stop word list.  I left the document text in its original case (most words capitalized the first letter), and not all stop words in the titles are lower-case.  I think there is more work to do on the stop words.  In my final feature space, some of the stop words appear to be useful terms (Bill, Fire).  And there are other terms I should exclude (e.g., Push, Use/Uses/Using) since they do not add to the understanding of where AI is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - Results and Next Steps\n",
    "I am quite pleased with the results of analyzing the titles of articles in the newsletters.  I was able to adjust parameters of the vectorization process to generate a useful feature space – keywords for the most prevalent topics (about 40) to a complete list of terms (around 1000).  I found it interesting to take a term and look up the related article titles and then sometimes dive into the newsletter article.  I can imagine creating a word cloud and being able to click on a word and see a list of the related article title(s) and then drilling through to the article.  My boss and I could spend hours engrossed in exploring the articles in this manner.  And that would spur us to want to take the next step and do more analysis of the text within the articles in a more automated fashion.\n",
    " \n",
    "My boss is pleased with the progress to date and encouraged me to keep going.  To do a more worthwhile analysis of the actual articles will probably require additional text mining techniques beyond count vectorization.  That is, identify the companies involved, how AI is being applied, and whether the effort is successful or not.  I would propose some exploratory analysis to get my arms around the type of content in the articles.  And then we would identify a couple of things we feel would be valuable to glean from the articles. And we would determine the practicality of biting off that portion of the text mining project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module(s) into namespace\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000) #important for getting all the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "- [Count Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "- [TfidVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1A - Create a Data Frame Containing Text from a Website\n",
    "\n",
    "On **Thursday, August 29, 2019**, I captured the contents of **the \n",
    "[WSJ PRO Artificial Intelligence Newsletter](https://www.wsj.com/pro/artificial-intelligence/newsletters) \n",
    "index page** and saved it to a local HTML file.  This index page listed the date and subject of available newsletters along with a link to the contents of each newsletter.  A newsletter usually contains 4-6 short articles or summary of a full article on the WSJ or other news/academic website.  The newsletters are typically delivered as an email but archives are still available on the WSJ's website.\n",
    "\n",
    "I \"scraped\" the title of articles from the subject line of the newsletter. The subject line usually contained the titles of three articles in the newsletter.\n",
    "\n",
    "**My corpus is the set of article titles in the subject line of the WSJ PRO Articial Intelligence daily newsletter from Jan 10 to Aug 29, 2019**. There are a total of **474 documents (article titles)** from 160 newsletters across nearly 8 months.\n",
    "\n",
    "For a future assignment, I hope to use the newsletter links I captured to retrieve and scrap the contents of each newsletter and have a richer set of text to analyze (4-6 short or summary articles from each newsletter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html lang=\"en\"><head><script type=\"text/javascript\" src=\"https://cdn.krxd.net/userdata/get?pub=fd886588-e5a4-4fe2-9d09-006f6d3c3aab&amp;callback=Krux.ns.dowjones.kxjsonp_userdata\"></script><script type=\"text/javascript\" src=\"https://consumer.krxd.net/consent/get/fd886588-e5a4-4fe2-9d09-006f6d3c3aab?idt=device&amp;dt=kxcookie&amp;callback=Krux.ns.dowjones.kxjsonp_consent_get_0\"></script><script type=\"text/javascript\" src=\"https://beacon.krxd.net/optout_check?callback=Krux.ns.dowjones.kxjsonp_optOutCheck\"></script><script type=\"text/javascript\" async=\"\" src=\"https://scdn.cxense.com/cx.js\"></script>\n",
      "      <meta charset=\"utf-8\">\n",
      "      <style>.WSJProTheme--fontSmoothing--3midT9i1{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}.WSJProTheme--grayBackground--27-wtIVs{background-color:#f1f0ee}.WSJProTheme--cyber-research--uVjg019B{border-top:2px solid #c7b78f}.WSJProTheme--cyber-research--uVjg019B.WSJProTheme--even--3qyDUt2S{background-color:#192f35}.WSJProTheme--cyber-research--uVjg019B.WSJProTheme--odd--pHTLQ647{background-color:#214046}.WSJProTheme--list-reset--3VtuwXka{padding:0;margin:0;display:block;list-style:none}.WSJProTheme--margin-left--31f6o5q3{margin-left:10px}.WSJProTheme--margin-right--DUwUCBYL{margin-right:10px}.WSJProTheme--margin-top--3h8T5Zn8{margin-top:10px}.WSJProTheme--margin-bottom--27X4NstR{margin-bottom:10px}.WSJProTheme--margin-left-large--2Ogz1N6K{margin-left:20px}.WSJProTheme--margin-right-large--2gvTAshg{margin-right:20px}.WSJProThe\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# read local copy of the target .html page into a variable \n",
    "# (instead of fetching the page from its source website)\n",
    "# -------------------------------------------------------------------------\n",
    "page_text = \"\"\n",
    "\n",
    "f = open(\"WSJ_Pro_AI_Newletters_20190829.html\", \"r\")\n",
    "page_text = f.read()\n",
    "f.close()\n",
    "\n",
    "# a sample of the text in the .html file\n",
    "print(page_text[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Scrapping Part 1 - extract info about each newsletter from the webpage\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# date info extracted 8/29/2019\n",
    "extracted_date_value = datetime.date(2019,8,29)\n",
    "\n",
    "# lists to hold data as extracted\n",
    "extract_date       = []\n",
    "newsletter_date    = []\n",
    "newsletter_subject = []\n",
    "newsletter_link    = []\n",
    "title1 = []\n",
    "title2 = []\n",
    "title3 = []\n",
    "\n",
    "# parse webpage with BeautifulSoup\n",
    "soup = BeautifulSoup(page_text, \"lxml\")\n",
    "\n",
    "# find section of interest on the webpage \n",
    "# the list or index of past WSJ PRO Artificial Intelligence newsletters\n",
    "bullet_list = soup.find(class_=\"WSJProTheme--article-bullet-list--3zTExlhb WSJProTheme--newsletters-section--13sYE7Nu\")\n",
    "\n",
    "# extract the date of newsletter (e.g., Thursday, August 29th)\n",
    "for tag in bullet_list.find_all(class_='WSJProTheme--date--HLkJMwSm'):\n",
    "    newsletter_date.append(tag.get_text())\n",
    "    extract_date.append(extracted_date_value)\n",
    "\n",
    "# extract the newletter subject text and link to contents of the newsletter \n",
    "for tag in bullet_list.find_all('a'):\n",
    "    subject = tag.get_text()\n",
    "    \n",
    "    # clean up subject - remove common intro text and standardize single quotes to the same character.\n",
    "    subject = subject.replace('Artificial Intelligence Daily: ', '')\n",
    "    subject = subject.replace(\"’\",\"'\")\n",
    "    subject = subject.replace(\"‘\",\"'\")\n",
    "    \n",
    "    newsletter_subject.append(subject)\n",
    "    newsletter_link.append(tag.get('href'))\n",
    "    \n",
    "    # split out titles of articles within newsletter subject line and store in their own column\n",
    "    # there are usually 3 titles separated by a semi-colon\n",
    "    article_titles = subject.split('; ')\n",
    "    title1.append('' if len(article_titles) <= 0 else article_titles[0])\n",
    "    title2.append('' if len(article_titles) <= 1 else article_titles[1])\n",
    "    title3.append('' if len(article_titles) <= 2 else article_titles[2])\n",
    "\n",
    "# create DataFrame\n",
    "newsletter_df = pd.DataFrame({\"Date\" : newsletter_date,\n",
    "                           \"Subject\" : newsletter_subject,\n",
    "                           \"Link\"    : newsletter_link,\n",
    "                           \"Title_1\" : title1,\n",
    "                           \"Title_2\" : title2,\n",
    "                           \"Title_3\" : title3,\n",
    "                           \"Extract_Date\" : extract_date\n",
    "                           })\n",
    "\n",
    "# I want to start with Newsletters from January 10th forward\n",
    "# This drops out a few of the early items that are included in the Newsletter index page\n",
    "\n",
    "first_one = newsletter_df.Date[newsletter_df.Date == 'Thursday, January 10th'].index[0]\n",
    "newsletter_df = newsletter_df.iloc[:first_one+1]\n",
    "\n",
    "# There are a couple duplicate entries - same date and subject, but different URL \n",
    "# However, URLs generate the same newsletter content.\n",
    "# It looks this happened on weeks with/or around a holiday\n",
    "\n",
    "newsletter_df.drop_duplicates(subset=\"Date\", keep='first', inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# # UNCOMMENT! TO SAVE CONTENTS TO .CSV FILE (when desired)\n",
    "# # save result to a file\n",
    "# newsletter_df.to_csv('newsletter_links.csv', index=False)\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title_1</th>\n",
       "      <th>Title_2</th>\n",
       "      <th>Title_3</th>\n",
       "      <th>Extract_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thursday, August 29th</td>\n",
       "      <td>ThoughtSpot Raises $248 Million; TSA to Test Facial Recognition in Vegas; Toyota, Suzuki Team Up</td>\n",
       "      <td>http://createsend.com/t/d-79AF8DD1B5FB119E2540EF23F30FEDED</td>\n",
       "      <td>ThoughtSpot Raises $248 Million</td>\n",
       "      <td>TSA to Test Facial Recognition in Vegas</td>\n",
       "      <td>Toyota, Suzuki Team Up</td>\n",
       "      <td>2019-08-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wednesday, August 28th</td>\n",
       "      <td>Linking EKG Data to Health Status; Ex-Google Engineer Charged in Theft Case; Synchrony's AI Push</td>\n",
       "      <td>http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED</td>\n",
       "      <td>Linking EKG Data to Health Status</td>\n",
       "      <td>Ex-Google Engineer Charged in Theft Case</td>\n",
       "      <td>Synchrony's AI Push</td>\n",
       "      <td>2019-08-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tuesday, August 27th</td>\n",
       "      <td>Robots May Soon Perform Home Appraisals; Helping Kids Deal With AI; Pony.ai and Toyota Team Up</td>\n",
       "      <td>http://createsend.com/t/d-B503896AA29F56592540EF23F30FEDED</td>\n",
       "      <td>Robots May Soon Perform Home Appraisals</td>\n",
       "      <td>Helping Kids Deal With AI</td>\n",
       "      <td>Pony.ai and Toyota Team Up</td>\n",
       "      <td>2019-08-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date  \\\n",
       "0   Thursday, August 29th   \n",
       "1  Wednesday, August 28th   \n",
       "2    Tuesday, August 27th   \n",
       "\n",
       "                                                                                            Subject  \\\n",
       "0  ThoughtSpot Raises $248 Million; TSA to Test Facial Recognition in Vegas; Toyota, Suzuki Team Up   \n",
       "1  Linking EKG Data to Health Status; Ex-Google Engineer Charged in Theft Case; Synchrony's AI Push   \n",
       "2    Robots May Soon Perform Home Appraisals; Helping Kids Deal With AI; Pony.ai and Toyota Team Up   \n",
       "\n",
       "                                                         Link  \\\n",
       "0  http://createsend.com/t/d-79AF8DD1B5FB119E2540EF23F30FEDED   \n",
       "1  http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED   \n",
       "2  http://createsend.com/t/d-B503896AA29F56592540EF23F30FEDED   \n",
       "\n",
       "                                   Title_1  \\\n",
       "0          ThoughtSpot Raises $248 Million   \n",
       "1        Linking EKG Data to Health Status   \n",
       "2  Robots May Soon Perform Home Appraisals   \n",
       "\n",
       "                                    Title_2                     Title_3  \\\n",
       "0   TSA to Test Facial Recognition in Vegas      Toyota, Suzuki Team Up   \n",
       "1  Ex-Google Engineer Charged in Theft Case         Synchrony's AI Push   \n",
       "2                 Helping Kids Deal With AI  Pony.ai and Toyota Team Up   \n",
       "\n",
       "  Extract_Date  \n",
       "0   2019-08-29  \n",
       "1   2019-08-29  \n",
       "2   2019-08-29  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A peek at info extracted in Part 1\n",
    "newsletter_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Scrapping Part 2 - Create Corpus of Article Titles (up to 3 from each newsletter)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# melt the Title_N columns into rows with a single Title column\n",
    "# So we have ... [Extract_Date, Date, Link, Title_Nbr (i.e., the source column), Title]\n",
    "\n",
    "titles_df = newsletter_df[['Date', 'Link', 'Title_1', 'Title_2', 'Title_3', 'Extract_Date']].melt(\n",
    "    id_vars=['Extract_Date', 'Date', 'Link'], \n",
    "    var_name='Title_Nbr', value_name='Title')\n",
    "\n",
    "# exclude empty titles - we do not always have 3 titles\n",
    "titles_df = titles_df[titles_df.Title!='']\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# # UNCOMMENT! TO SAVE CONTENTS TO .CSV FILE (when desired)\n",
    "# # save result to a file\n",
    "# titles_df.to_csv('article_titles.csv', index=False)\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * * Result of Task 1:  Head & Tail of DataFrame * * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Corpus is  (474, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extract_Date</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title_Nbr</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Thursday, August 29th</td>\n",
       "      <td>http://createsend.com/t/d-79AF8DD1B5FB119E2540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>ThoughtSpot Raises $248 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Wednesday, August 28th</td>\n",
       "      <td>http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>Linking EKG Data to Health Status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Tuesday, August 27th</td>\n",
       "      <td>http://createsend.com/t/d-B503896AA29F56592540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>Robots May Soon Perform Home Appraisals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Monday, August 26th</td>\n",
       "      <td>http://createsend.com/t/d-5CCCECDBA202FB562540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>Huawei Unveils AI Chip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Friday, August 23rd</td>\n",
       "      <td>http://createsend.com/t/d-B1E4712AB017D6162540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>Executives Offer Lessons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Extract_Date                    Date  \\\n",
       "0   2019-08-29   Thursday, August 29th   \n",
       "1   2019-08-29  Wednesday, August 28th   \n",
       "2   2019-08-29    Tuesday, August 27th   \n",
       "3   2019-08-29     Monday, August 26th   \n",
       "4   2019-08-29     Friday, August 23rd   \n",
       "\n",
       "                                                         Link Title_Nbr  \\\n",
       "0  http://createsend.com/t/d-79AF8DD1B5FB119E2540EF23F30FEDED   Title_1   \n",
       "1  http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED   Title_1   \n",
       "2  http://createsend.com/t/d-B503896AA29F56592540EF23F30FEDED   Title_1   \n",
       "3  http://createsend.com/t/d-5CCCECDBA202FB562540EF23F30FEDED   Title_1   \n",
       "4  http://createsend.com/t/d-B1E4712AB017D6162540EF23F30FEDED   Title_1   \n",
       "\n",
       "                                     Title  \n",
       "0          ThoughtSpot Raises $248 Million  \n",
       "1        Linking EKG Data to Health Status  \n",
       "2  Robots May Soon Perform Home Appraisals  \n",
       "3                   Huawei Unveils AI Chip  \n",
       "4                 Executives Offer Lessons  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape of Corpus is \", titles_df.shape)\n",
    "titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extract_Date</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title_Nbr</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Friday, January 18th</td>\n",
       "      <td>http://createsend.com/t/d-7FCF99F8B4638E8B2540EF23F30FEDED</td>\n",
       "      <td>Title_3</td>\n",
       "      <td>New Arms Race</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Thursday, January 17th</td>\n",
       "      <td>http://createsend.com/t/d-9D894661119CA9AD2540EF23F30FEDED</td>\n",
       "      <td>Title_3</td>\n",
       "      <td>GE Adopts 'Humble AI'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Wednesday, January 16th</td>\n",
       "      <td>http://createsend.com/t/d-C76FFC4D1B068C022540EF23F30FEDED</td>\n",
       "      <td>Title_3</td>\n",
       "      <td>Combating Bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Friday, January 11th</td>\n",
       "      <td>http://createsend.com/t/d-CB7B7296A8CC3A422540EF23F30FEDED</td>\n",
       "      <td>Title_3</td>\n",
       "      <td>The Legal Angle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Thursday, January 10th</td>\n",
       "      <td>http://createsend.com/t/d-82684E8CDC8A11A02540EF23F30FEDED</td>\n",
       "      <td>Title_3</td>\n",
       "      <td>Symantec 'In-Sources' Data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Extract_Date                     Date  \\\n",
       "473   2019-08-29     Friday, January 18th   \n",
       "474   2019-08-29   Thursday, January 17th   \n",
       "475   2019-08-29  Wednesday, January 16th   \n",
       "478   2019-08-29     Friday, January 11th   \n",
       "479   2019-08-29   Thursday, January 10th   \n",
       "\n",
       "                                                           Link Title_Nbr  \\\n",
       "473  http://createsend.com/t/d-7FCF99F8B4638E8B2540EF23F30FEDED   Title_3   \n",
       "474  http://createsend.com/t/d-9D894661119CA9AD2540EF23F30FEDED   Title_3   \n",
       "475  http://createsend.com/t/d-C76FFC4D1B068C022540EF23F30FEDED   Title_3   \n",
       "478  http://createsend.com/t/d-CB7B7296A8CC3A422540EF23F30FEDED   Title_3   \n",
       "479  http://createsend.com/t/d-82684E8CDC8A11A02540EF23F30FEDED   Title_3   \n",
       "\n",
       "                          Title  \n",
       "473               New Arms Race  \n",
       "474       GE Adopts 'Humble AI'  \n",
       "475              Combating Bias  \n",
       "478             The Legal Angle  \n",
       "479  Symantec 'In-Sources' Data  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Apply a Count Vectorizer & Create a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Steps for Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Load corpus from file, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Corpus is  (474, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extract_Date</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title_Nbr</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Thursday, August 29th</td>\n",
       "      <td>http://createsend.com/t/d-79AF8DD1B5FB119E2540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>ThoughtSpot Raises $248 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Wednesday, August 28th</td>\n",
       "      <td>http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>Linking EKG Data to Health Status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Tuesday, August 27th</td>\n",
       "      <td>http://createsend.com/t/d-B503896AA29F56592540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>Robots May Soon Perform Home Appraisals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Monday, August 26th</td>\n",
       "      <td>http://createsend.com/t/d-5CCCECDBA202FB562540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>Huawei Unveils AI Chip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>Friday, August 23rd</td>\n",
       "      <td>http://createsend.com/t/d-B1E4712AB017D6162540EF23F30FEDED</td>\n",
       "      <td>Title_1</td>\n",
       "      <td>Executives Offer Lessons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Extract_Date                    Date  \\\n",
       "0   2019-08-29   Thursday, August 29th   \n",
       "1   2019-08-29  Wednesday, August 28th   \n",
       "2   2019-08-29    Tuesday, August 27th   \n",
       "3   2019-08-29     Monday, August 26th   \n",
       "4   2019-08-29     Friday, August 23rd   \n",
       "\n",
       "                                                         Link Title_Nbr  \\\n",
       "0  http://createsend.com/t/d-79AF8DD1B5FB119E2540EF23F30FEDED   Title_1   \n",
       "1  http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED   Title_1   \n",
       "2  http://createsend.com/t/d-B503896AA29F56592540EF23F30FEDED   Title_1   \n",
       "3  http://createsend.com/t/d-5CCCECDBA202FB562540EF23F30FEDED   Title_1   \n",
       "4  http://createsend.com/t/d-B1E4712AB017D6162540EF23F30FEDED   Title_1   \n",
       "\n",
       "                                     Title  \n",
       "0          ThoughtSpot Raises $248 Million  \n",
       "1        Linking EKG Data to Health Status  \n",
       "2  Robots May Soon Perform Home Appraisals  \n",
       "3                   Huawei Unveils AI Chip  \n",
       "4                 Executives Offer Lessons  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"article_titles.csv\"\n",
    "titles_df = pd.read_csv(filename) \n",
    "\n",
    "print(\"Shape of Corpus is \", titles_df.shape)\n",
    "titles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Create Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Given data from a vectorizer, return a sorted list of features \n",
    "# -------------------------------------------------------------------------\n",
    "def get_sorted_features(features_array, features_names):\n",
    "    # count the number of documents where the feature exists\n",
    "    document_count = np.sum(features_array, axis=0) \n",
    "\n",
    "        # convert list to array to sum up feature existence (0,1) (i.e., sum for each column)\n",
    "        # note: (axis=0 calc/sum by column, axis=1 calc by row)\n",
    "\n",
    "    df = pd.DataFrame(document_count.tolist(), index=features_names, columns=['Documents'])\n",
    "\n",
    "        # create a dataframe from the list (note: converting numpy array 'document_count' to a list)\n",
    "\n",
    "    return df.sort_values(['Documents'], ascending = False) \n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# for given feature, return original data from corpus\n",
    "# -------------------------------------------------------------------------\n",
    "def search_titles(feature):\n",
    "    search_token_pattern = '(?u)\\\\b{}\\\\b'.format(feature)\n",
    "    return titles_df[titles_df.Title.str.contains(search_token_pattern, regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 1 - Default - Feature Exists\n",
    "\n",
    "As a first pass, explore the 1-word features that exist in the article titles.  At this point, just calculate if words exist (`binary=True`) rather than frequency or weights.\n",
    "\n",
    "**Results:**  For 474 documents (article titles), the default vectorizer produced **1085 features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of bag of words:  (474, 1085)\n",
      "Features Detected:  1085\n"
     ]
    }
   ],
   "source": [
    "cv_words_exist = CountVectorizer(binary=True) \n",
    "\n",
    "titles_words_exist = cv_words_exist.fit_transform(titles_df['Title'])\n",
    "\n",
    "# print(type(titles_words_exist))\n",
    "\n",
    "print(\"Shape of bag of words: \", titles_words_exist.shape)\n",
    "print(\"Features Detected: \", titles_words_exist.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Features - Most Frequent Across Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognition</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facial</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robots</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethics</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Documents\n",
       "ai                 180\n",
       "to                  73\n",
       "the                 31\n",
       "in                  26\n",
       "for                 26\n",
       "of                  23\n",
       "on                  20\n",
       "with                20\n",
       "recognition         19\n",
       "facial              18\n",
       "tech                18\n",
       "data                15\n",
       "google              15\n",
       "robots              14\n",
       "ethics              13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of the features identified\n",
    "features_names = cv_words_exist.get_feature_names()\n",
    "\n",
    "sorted_count_df = get_sorted_features(titles_words_exist.toarray(), features_names)\n",
    "sorted_count_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on Top Features  \n",
    "The list above is the feature and number of documents (article titles) in which the term is found.\n",
    "\n",
    "**'ai'** is the top feature in 180 article titles (of 474 or 38%).  **Makes sense!**  \n",
    "The next features look like stop word candidates (to, the, in, for, of, on, with).  \n",
    "The 9th feature and beyond look like appropriate terms from the article titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', '11', '16', '2022', '248', '40', '50', 'ab', 'abandon', 'abroad', 'absolut', 'abuse', 'academic', 'academics', 'acceptance', 'accidents', 'account', 'accountants', 'acquisition', 'acquisitions', 'across', 'add', 'adding', 'address', 'addressing', 'adoption', 'adopts', 'ads', 'advance', 'adversarial', 'advise', 'against', 'agency', 'ahead', 'ai', 'aim', 'aims', 'airbus', 'airline', 'airport', 'alexa', 'algo', 'algorithm', 'algorithms', 'algos', 'alien', 'all', 'alphasense', 'amazon', 'among', 'an', 'analysis', 'analytics', 'and', 'andrew', 'angle', 'angles', 'animal', 'anthem', 'antibias', 'antitrust', 'anxiety', 'app', 'apple', 'applicants', 'appoints', 'appraisals', 'approach', 'approve', 'architecture', 'are', 'argoai', 'arise', 'arms', 'as', 'assault', 'assessing', 'asset', 'assistant', 'assistants', 'at', 'athletic', 'attacks', 'auditing', 'augments', 'autism', 'auto', 'automating', 'automation', 'autonomous', 'avoid', 'avoiding', 'aws', 'back', 'backlash', 'balancing', 'ban', 'bank', 'banks', 'bans', 'barriers', 'based', 'be', 'behind', 'benchmarks', 'benefit', 'benefits', 'benevolent', 'best', 'bet', 'better', 'bias', 'big', 'bill', 'billion', 'bills', 'biometrics', 'black', 'bluffs', 'bluvector', 'board', 'boards', 'body', 'bolster', 'bolsters', 'boom', 'boost', 'boosting', 'boosts', 'boston', 'bot', 'bots', 'bowl', 'box', 'brain', 'brakes', 'breach', 'breakpoint', 'breakup', 'bridges', 'bubble', 'build', 'builds', 'bulks', 'buses', 'business', 'businesses', 'buy', 'buys', 'by', 'cables', 'california', 'call', 'calls', 'can', 'canada', 'cancer', 'capitalizing', 'car', 'carbon', 'carbonite', 'care', 'careerbuilder', 'caregiving', 'cars', 'case', 'cashierless', 'casino', 'catch', 'catching', 'cbp', 'center', 'centers', 'centralizing', 'ceo', 'certificate', 'cfos', 'chain', 'challenge', 'challenges', 'change', 'changing', 'charged', 'charges', 'chatbot', 'chatbots', 'cheaper', 'cheaters', 'checking', 'chief', 'china', 'chip', 'choices', 'christchurch', 'cia', 'cios', 'circuit', 'cite', 'city', 'cleanup', 'client', 'climate', 'clinics', 'closer', 'cloud', 'co', 'collaboration', 'collar', 'college', 'collude', 'combat', 'combating', 'comcast', 'comes', 'coming', 'command', 'comment', 'communication', 'companies', 'company', 'compensation', 'compete', 'complex', 'computing', 'conagra', 'concerns', 'congress', 'construction', 'consultancy', 'consultant', 'continental', 'contractors', 'cook', 'corporate', 'corporations', 'cosmetics', 'cost', 'costs', 'could', 'council', 'course', 'courses', 'creates', 'creating', 'creator', 'creditworthiness', 'cruising', 'crunching', 'curb', 'customer']\n"
     ]
    }
   ],
   "source": [
    "# explore list - just scan the top and bottom parts of the list\n",
    "\n",
    "features_names = cv_words_exist.get_feature_names()\n",
    "\n",
    "#features_names[:501]  # approximately top half of list\n",
    "#features_names[501:]  # bottom half\n",
    "\n",
    "print(features_names[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts from reviewing the 1085 Features\n",
    "\n",
    "- Some numbers: 100, 11, 248\n",
    "- Various similar forms of same word\n",
    "  - academic, academics\n",
    "  - acquisition, acquisitions\n",
    "  - algo, algorithm, algorithms, algos\n",
    "  - ...\n",
    "- Probably need some 2-word features\n",
    "  - andrew, ng\n",
    "- Some additional stop words\n",
    "  - an, are, as, at\n",
    "  - ?'s - could, your\n",
    "- Current vectorizer is splitting hyphenated words such as word Ex-Google into 'ex' and 'google'\n",
    "- There are some quoted terms in the titles, I wonder if those should be extracted as a single feature.  (e.g., 'Ethical', 'In-Sources', 'Humble AI').  Probably not important. I suspect they would be unique (not repeated in many titles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many words (features) per article title (document)?\n",
    "\n",
    "**Results:** On average 4-5, IQR 3-6 words, min just 2 and max 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nbr_of_Words\n",
      "count    474.000000\n",
      "mean       4.670886\n",
      "std        1.586584\n",
      "min        2.000000\n",
      "25%        3.000000\n",
      "50%        5.000000\n",
      "75%        6.000000\n",
      "max       10.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x1a1fce4470>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_words_in_title = np.sum(titles_words_exist.toarray(), axis=1)\n",
    "    # sum by row - words per title\n",
    "\n",
    "count_words_df = pd.DataFrame(count_words_in_title.tolist(), columns=['Nbr_of_Words'])\n",
    "\n",
    "# summary statistics\n",
    "print(count_words_df.describe())\n",
    "\n",
    "# histogram\n",
    "count_words_df.hist(bins=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 1-A - Default - Feature Frequency\n",
    "#### Are words duplicated within an article title (document)? \n",
    "I would guess very few, given the short article titles (3-6 words)\n",
    "\n",
    "**Results:** NO!  'to' (a stop word) is the only word that appears in a title more than once.\n",
    "\n",
    "So going forward, I will continue with just looking at feature exists in document (`binary=True`) rather calculate the actual frequency that a feature occurs within a document (`binary=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create/apply vectorizer to return counts of words, rather than 0/1 for exist of word.\n",
    "cv_nbr_words = CountVectorizer(binary=False)  \n",
    "\n",
    "titles_nbr_words = cv_nbr_words.fit_transform(titles_df['Title'])\n",
    "\n",
    "\n",
    "# typical bag of words DataFrame\n",
    "# - rows    - document\n",
    "# - columns - the words/features across the corpus\n",
    "# - cell    - count of occurrence of word in document\n",
    "nbr_words_df = pd.DataFrame(titles_nbr_words.toarray(), columns=cv_nbr_words.get_feature_names())\n",
    "\n",
    "# find max number of occurrences of each word\n",
    "max_occurrence = nbr_words_df.max()\n",
    "max_occurrence[max_occurrence > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 2 - Preserve Original Case\n",
    "\n",
    "The text (article title) is 'title cased' where most words are capitalized, except stop words.  **I suspect** keeping the original case will not dramatically increase the number of features.  Plus, some of the additional features will likely be eliminated as stop words.\n",
    "\n",
    "**Results:**  Preserving the original case (i.e., do not convert to lower case) is helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Detected:  1098\n"
     ]
    }
   ],
   "source": [
    "cv_mixed_case = CountVectorizer(binary=True, lowercase=False) \n",
    "titles_mixed_case = cv_mixed_case.fit_transform(titles_df['Title'])\n",
    "\n",
    "print(\"Features Detected: \", titles_mixed_case.shape[1])\n",
    "\n",
    "# 1098 case sensitive features vs. 1085 lower case features\n",
    "# difference of (13 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Worker',\n",
       " 'Workers',\n",
       " 'Workforce',\n",
       " 'Worry',\n",
       " 'Wrestles',\n",
       " 'Your',\n",
       " 'Zappos',\n",
       " 'Zone',\n",
       " 'ai',\n",
       " 'an',\n",
       " 'and',\n",
       " 'as',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'in',\n",
       " 'into',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'the',\n",
       " 'to',\n",
       " 'with']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore list\n",
    "features_names = cv_mixed_case.get_feature_names()\n",
    "\n",
    "features_names[1075:]  # bottom of list - most likely the lower case variants\n",
    "\n",
    "# features_names[:550]   # top    half of list\n",
    "# features_names[550:]   # bottom half of list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on Results\n",
    "\n",
    "Preserving the original case in the article titles generated 1098 features (or 13 additional). Converting text to all lower case created 1085 features.\n",
    "\n",
    "The additional features (all lower cases vs. first letter capitalized) should be at the bottom of the list.  Other than 'ai' they do look like stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 3 - Adjust RegEx to Preserve Hyphenated Words\n",
    "The default regular expression for a token splits hyphenated words at the hyphen.  About 10% of the titles contain a hyphenated word.  I believe it is more meaningful to keep the hyphenated word as a feature (single word) than separate into the individual components.  Example of hyphenated words: High-Tech, Self-Sufficiency, Co-Founder, White-Collar, Open-Source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Detected:  1107\n",
      "\n",
      "\n",
      "['100', '11.5', '16', '2022', '248', '40', '50', 'AB', 'AI', 'AI-Based', 'AI-Driven', 'AI-Powered', 'AT', 'AWS', 'Abandon', 'Abroad', 'Absolut', 'Abuse', 'Academic', 'Academics', 'Acceptance', 'Accidents', 'Account', 'Accountants', 'Acquisition', 'Acquisitions', 'Across', 'Add', 'Adding', 'Address', 'Addressing', 'Adoption', 'Adopts', 'Ads', 'Advance', 'Adversarial', 'Advise', 'Against', 'Agency', 'Ahead', 'Aim', 'Aims', 'Airbus', 'Airline', 'Airport', 'Alexa', 'Algo', 'Algo-Powered', 'Algorithm', 'Algorithms', 'Algos', 'Alien', 'All', 'AlphaSense', 'Amazon', 'Among', 'Analysis', 'Analytics', 'Andrew', 'Angle', 'Angles', 'Animal-Recognition', 'Anthem', 'Antibias', 'Antitrust', 'Anxiety', 'App', 'Apple', 'Apple-Picking', 'Applicants', 'Appoints', 'Appraisals', 'Approach', 'Approve', 'Architecture', 'Are', 'ArgoAI', 'Arise', 'Arms', 'Assault', 'Assessing', 'Asset', 'Assistant', 'Assistants', 'At', 'Athletic', 'Attacks', 'Auditing', 'Augments', 'Autism', 'Auto', 'Automating', 'Automation', 'Autonomous', 'Avoid', 'Avoiding', 'Back', 'Backlash', 'Balancing', 'Ban', 'Bank', 'Banks', 'Bans', 'Barriers', 'Be', 'Behind', 'Benchmarks', 'Benefit', 'Benefits', 'Benevolent', 'Best', 'Bet', 'Better', 'Bias', 'Big', 'Bill', 'Billion', 'Bills', 'Biometrics', 'Black', 'BluVector', 'Bluffs', 'Board', 'Boards', 'Body', 'Bolster', 'Bolsters', 'Boom', 'Boost', 'Boosting', 'Boosts', 'Boston', 'Bot', 'Bots', 'Bowl', 'Box', 'Brain', 'Brakes', 'Breach', 'Breakpoint', 'Breakup', 'Bridges', 'Bubble', 'Build', 'Builds', 'Bulks', 'Buses', 'Business', 'Businesses', 'Buy', 'Buys', 'CBP', 'CEO', 'CFOs', 'CIA', 'CIOs', 'Cables', 'California', 'Call', 'Calls', 'Can', 'Canada', 'Cancer-Diagnosis', 'Capitalizing', 'Car', 'Carbon', 'Carbonite', 'Care', 'CareerBuilder', 'Caregiving', 'Cars', 'Case', 'Cashierless', 'Casino', 'Catch', 'Catching', 'Center', 'Centers', 'Centralizing', 'Certificate', 'Chain', 'Challenge', 'Challenges', 'Change', 'Changing', 'Charged', 'Charges', 'Chatbot', 'Chatbots', 'Cheaper', 'Cheaters', 'Chief', 'China', 'Chip', 'Choices', 'Christchurch', 'Circuit-Board-Size', 'Cite', 'City', 'Cleanup', 'Client', 'Climate', 'Clinics', 'Closer', 'Cloud', 'Co-Founder', 'Collaboration', 'College', 'Collude', 'Combat', 'Combating', 'Comcast', 'Comes', 'Coming', 'Command', 'Comment', 'Communication', 'Companies', 'Company', 'Compensation', 'Compete', 'Complex', 'Computing', 'Conagra', 'Concerns', 'Congress', 'Construction', 'Consultancy', 'Consultant', 'Continental', 'Contractors', 'Cook', 'Corporate', 'Corporations', 'Cosmetics', 'Cost', 'Cost-Cutting', 'Costs', 'Could', 'Council', 'Course', 'Courses', 'Creates', 'Creating', 'Creator', 'Creditworthiness', 'Cruising', 'Curb', 'Customer', 'Customer-Service']\n"
     ]
    }
   ],
   "source": [
    "# I found tips on adjusting the regex of the token from ...\n",
    "# https://stackoverflow.com/questions/38115367/scikit-learn-dont-separate-hyphenated-words-while-tokenization\n",
    "\n",
    "# default token pattern is (?u)\\b\\w\\w+\\b\n",
    "    # note:  \\w = [a-zA-Z0-9_]\n",
    "\n",
    "# my_token_pattern = \"(?u)\\\\b\\\\w\\\\w+\\\\b\"          # default\n",
    "# my_token_pattern = \"(?u)\\\\b\\\\w[\\\\w\\\\-]+\\\\b\"     # keep hyphen\n",
    "my_token_pattern = \"(?u)\\\\b\\\\w[\\\\w\\\\-.]+\\\\b\"    # keep hyphen & period (for abbreviations)\n",
    "\n",
    "    # hmmm, it does not capture the ending period (e.g., U.S vs U.S. or Sen vs. Sen.)\n",
    "\n",
    "cv3 = CountVectorizer(binary=True, lowercase=False, token_pattern=my_token_pattern) \n",
    "\n",
    "features_cv3 = cv3.fit_transform(titles_df['Title'])\n",
    "\n",
    "print(\"Features Detected: \", features_cv3.shape[1])\n",
    "print(\"\\n\")\n",
    "\n",
    "#1098 # default - at least 2 char word \"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "#1104 # include hyphen w/in words      \"(?u)\\\\b\\\\w[\\\\w\\\\-]+\\\\b\"\n",
    "#1107 # also include period w/in words \"(?u)\\\\b\\\\w[\\\\w\\\\-.]+\\\\b\"\n",
    "    # like u.k, u.s, s.f\n",
    "\n",
    "# explore list\n",
    "features_names = cv3.get_feature_names()\n",
    "\n",
    "# features_names[:550]   # top    half of list\n",
    "# features_names[550:]   # bottom half of list\n",
    "\n",
    "print(features_names[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on Results\n",
    "\n",
    "The hyphenated words are less common (i.e., do not appear in many article titles).  But the hyphenated words are more meaningful features and I did not bloat my list of features.  I only added 5 total features by including the hyphen.  I suspect I am adding a hyphenated word but elminating or replacing one of the component pieces as a separate features.  For example, Ex-Google or Co-Founder I am probably eliminating Ex and Co as (useless) words/features.\n",
    "\n",
    "I also tried to capture abbreviations by including periods in my token. For example U.S., U.K. or S.F. were not originally included as a feature because the regex did not include periods.  These were broken into a token for each chunk between the periods and then excluded as single char tokens.  I don't quite have the right regex.  I'm not capturing the ending period.  But I'll leave that as a task for another time.\n",
    "\n",
    "The code below show the features my custom regex eliminated and the new features it was able to capture.\n",
    "\n",
    "**Perserving hyphenated words (and abbreviations, too) is helpful!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Features Elminated\n",
      "{'Fact', 'Hyper', 'Powered', 'Laundry', 'Driving', 'By', 'ai', 'Sector', 'Checking', 'Diagnosis', '11', 'Sufficiency', 'Image', 'Personalization', 'Senior', 'Crunching', 'Picking', 'Site', 'Space', 'Folding', 'Self', 'Labeling', 'Level', 'Sources', 'Source', 'Based', 'Pony', 'Cutting', 'Ex', 'Size', 'Co', 'Cancer', 'Paige', 'Like', 'Circuit', 'Animal', 'Collar', 'Driven', 'Gen'}\n",
      "\n",
      "48 Features Added\n",
      "{'White-Collar', 'Next-Gen', 'Self-Driving', 'Algo-Powered', 'Data-Labeling', 'Customer-Service', 'Facial-Recognition', 'Cost-Cutting', 'Job-Site', 'Human-Like', 'Co-Founder', 'Fire-Fighting', 'S.F', 'In-Sources', 'Human-Machine', 'Circuit-Board-Size', 'Data-Crunching', 'Death-Predicting', 'Fact-Checking', 'Office-Space', 'Manufacturing-Sector', 'Laundry-Folding', 'Paige.AI', 'AI-Driven', 'Open-Source', 'U.S', 'Next-Level', 'Cancer-Diagnosis', 'Senior-Housing', 'Image-Recognition', 'Pony.ai', 'AI-Based', '11.5', 'Facial-Image', 'Apple-Picking', 'Hyper-Personalization', 'U.K', 'AI-Powered', 'Animal-Recognition', 'Self-Regulation', 'On-Device', 'High-Tech', 'Self-Sufficiency', 'Machine-Learning', 'Pay-By-Face', 'Drive.ai', 'Ex-Google', 'Data-Mining'}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# look at differences in features\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "features_names_v2 = cv_mixed_case.get_feature_names()\n",
    "features_names_v3 = cv3.get_feature_names()\n",
    "\n",
    "# features removed/lost (in v2, but not v3)\n",
    "features_del = set(features_names_v2) - set(features_names_v3)\n",
    "print(\"{} Features Elminated\".format(len(features_del)))\n",
    "print(features_del)\n",
    "\n",
    "# features added (in v3, but not v2)\n",
    "features_add = set(features_names_v3) - set(features_names_v2)\n",
    "print(\"\\n{} Features Added\".format(len(features_add)))\n",
    "print(features_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 4 - Exclude Stop Words\n",
    "\n",
    "Now exclude stop words, on top of Variation 3 settings (preserves original case, custom regex to include hyphens and periods).\n",
    "\n",
    "I needed to create a custom list of stop words.  The default stop word list only contained lower case words.  I preserved the case of the token and sometimes stop words are actually capitalized in the article title.\n",
    "\n",
    "**Result:** Eliminated 72 features (6%) using terms on the default stop list.  Now I'm at 1035 features.\n",
    "There are a few features excluded that are a little questionable (Bill, Call, Fire).  More research is needed to decide if I should make some additional adjustments to the my custom list of stop words (add or remove words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with default stop words and include as all lower case and first letter capitalized\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "my_stop_words = set()\n",
    "for stopword in stop_words.ENGLISH_STOP_WORDS:\n",
    "    my_stop_words.add(stopword)\n",
    "    my_stop_words.add(stopword.capitalize())\n",
    "\n",
    "# print(len(my_stop_words))\n",
    "# 318 - double = 636\n",
    "# my_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Detected:  1035\n",
      "\n",
      "Words Excluded (e.g., a stop word)\n",
      "\n",
      "{'In', 'Too', 'Call', 'Your', 'Behind', 'More', 'Interest', 'Out', 'Its', 'Next', 'That', 'It', 'Still', 'On', 'to', 'With', 'Against', 'Among', 'Back', 'When', 'of', 'May', 'Up', 'with', 'Where', 'From', 'and', 'All', 'for', 'How', 'at', 'Are', 'Why', 'Could', 'as', 'Can', 'Than', 'Be', 'Bill', 'Off', 'Across', 'We', 'The', 'Side', 'an', 'At', 'For', 'on', 'Should', 'Might', 'in', 'into', 'Through', 'Is', 'Will', 'the', 'Get', 'Serious', 'Go', 'Fire', 'Over', 'Find', 'Less', 'or', 'by', 'Everywhere', 'Keep', 'What', 'Who', 'Into', 'See', 'Themselves'}\n"
     ]
    }
   ],
   "source": [
    "my_token_pattern = \"(?u)\\\\b\\\\w[\\\\w\\\\-.]+\\\\b\"    # keep hyphen & period (for abbreviations)\n",
    "\n",
    "cv4 = CountVectorizer(binary=True, lowercase=False,\n",
    "                      token_pattern=my_token_pattern, \n",
    "                      stop_words=my_stop_words) \n",
    "\n",
    "features_cv4 = cv4.fit_transform(titles_df['Title'])\n",
    "\n",
    "print(\"Features Detected: \", features_cv4.shape[1])\n",
    "\n",
    "#1098 # default - at least 2 char word \"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "#1107 # regex to include hyphen & period w/in words \"(?u)\\\\b\\\\w[\\\\w\\\\-.]+\\\\b\"\n",
    "#1093 # exclude stop words (default 'english' - all lower case)\n",
    "#1035 # exclude stop words (custom  lower case & capitalized version)\n",
    "\n",
    "# explore list\n",
    "features_names = cv4.get_feature_names()\n",
    "\n",
    "#features_names[:550]   # top    half of list\n",
    "#features_names[550:]   # bottom half of list\n",
    "\n",
    "\n",
    "# Look at words excluded (difference in features between Variation 3 and 4)\n",
    "# Do they appear to be stop words that make sense to eliminated?\n",
    "print(\"\\nWords Excluded (e.g., a stop word)\\n\")\n",
    "features_names_cv3 = cv3.get_feature_names()\n",
    "print(set(features_names_cv3) - set(features_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 4 Top Features (Most Frequent Across Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Google</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robots</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facial</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recognition</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ethics</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robot</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Use</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Startup</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Documents\n",
       "AI                 172\n",
       "Tech                15\n",
       "Google              14\n",
       "Robots              14\n",
       "Facial              14\n",
       "Recognition         14\n",
       "Ethics              13\n",
       "New                 13\n",
       "Data                12\n",
       "Robot               11\n",
       "Use                 10\n",
       "Health               9\n",
       "Business             9\n",
       "Startup              8\n",
       "Bias                 8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_count_df = get_sorted_features(features_cv4.toarray(), cv4.get_feature_names())\n",
    "sorted_count_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 5 - Prevalent Terms\n",
    "\n",
    "Building on Variation 4, expand to also include 2 word features and limit features to those in a minimum threshold of documents (1%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Detected:  42\n",
      "\n",
      " ['AI', 'Amazon', 'Automation', 'Bias', 'Big', 'Business', 'Change', 'China', 'Data', 'Driverless', 'Ethics', 'Facial', 'Facial Recognition', 'Firms', 'Google', 'Health', 'Helping', 'Human', 'IBM', 'Investment', 'Learning', 'Looks', 'Machine', 'Make', 'Making', 'Microsoft', 'Million', 'New', 'Push', 'Raises', 'Recognition', 'Robot', 'Robots', 'Seen', 'Startup', 'Tech', 'Tools', 'U.S', 'Use', 'Uses', 'Using', 'Using AI']\n"
     ]
    }
   ],
   "source": [
    "my_token_pattern = \"(?u)\\\\b\\\\w[\\\\w\\\\-.]+\\\\b\"    # keep hyphen & period (for abbreviations)\n",
    "\n",
    "cv5 = CountVectorizer(binary=True, lowercase=False,\n",
    "                      token_pattern=my_token_pattern, \n",
    "                      stop_words=my_stop_words,\n",
    "                      min_df=0.01,\n",
    "                      ngram_range=(1,2)\n",
    "                     ) \n",
    "\n",
    "features_cv5 = cv5.fit_transform(titles_df['Title'])\n",
    "\n",
    "print(\"Features Detected: \", features_cv5.shape[1])\n",
    "\n",
    "features_names = cv5.get_feature_names()\n",
    "\n",
    "print('\\n', features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 5 Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facial</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recognition</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Google</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robots</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facial Recognition</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ethics</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robot</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Use</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Startup</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Push</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seen</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Using</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Looks</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Make</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uses</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Million</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Investment</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Firms</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Change</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tools</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Automation</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Big</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U.S</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Helping</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Driverless</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Raises</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Microsoft</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Making</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Using AI</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Documents\n",
       "AI                        172\n",
       "Tech                       15\n",
       "Facial                     14\n",
       "Recognition                14\n",
       "Google                     14\n",
       "Robots                     14\n",
       "Facial Recognition         14\n",
       "Ethics                     13\n",
       "New                        13\n",
       "Data                       12\n",
       "Robot                      11\n",
       "Use                        10\n",
       "Business                    9\n",
       "Health                      9\n",
       "Startup                     8\n",
       "Bias                        8\n",
       "Push                        8\n",
       "Seen                        7\n",
       "Amazon                      7\n",
       "Using                       7\n",
       "Looks                       7\n",
       "China                       7\n",
       "Learning                    6\n",
       "Make                        6\n",
       "Uses                        6\n",
       "Million                     6\n",
       "Investment                  6\n",
       "IBM                         6\n",
       "Human                       6\n",
       "Machine                     6\n",
       "Firms                       6\n",
       "Change                      6\n",
       "Tools                       5\n",
       "Automation                  5\n",
       "Big                         5\n",
       "U.S                         5\n",
       "Helping                     5\n",
       "Driverless                  5\n",
       "Raises                      5\n",
       "Microsoft                   5\n",
       "Making                      5\n",
       "Using AI                    5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_count_df = get_sorted_features(features_cv5.toarray(), cv5.get_feature_names())\n",
    "sorted_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Finally!! Research Feature & Associated Article Title(s)\n",
    "\n",
    "Finally, take some features and look up the related article(s).\n",
    "\n",
    "A lot of the terms make sense.\n",
    "- Companies: Google, Amazon, IBM, Microsoft\n",
    "- Topics: Facial Recognition, Robot/Robots, Ethics, Bias\n",
    "\n",
    "Some seem more like stop words that do not add much to the understanding of the prevelant article topics.\n",
    "- New, Use/Using/Uses, Push, Seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Appropriate/Expected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linking EKG Data to Health Status</td>\n",
       "      <td>Wednesday, August 28th</td>\n",
       "      <td>http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Health Care, Sales Software Draw VC Funding</td>\n",
       "      <td>Tuesday, July 30th</td>\n",
       "      <td>http://createsend.com/t/d-A1BD7A3CB179A11F2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Health Agency Launches AI Challenge</td>\n",
       "      <td>Monday, April 1st</td>\n",
       "      <td>http://createsend.com/t/d-966FFE7CC9E33FD92540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Health Providers Develop AI Pathology Tools</td>\n",
       "      <td>Thursday, February 28th</td>\n",
       "      <td>http://createsend.com/t/d-CB60BD202A582E9F2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Safeway Expands AI Health Clinics</td>\n",
       "      <td>Wednesday, February 27th</td>\n",
       "      <td>http://createsend.com/t/d-F7543AEA82C99F6D2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Using AI and Wearables to Improve Health Care</td>\n",
       "      <td>Wednesday, May 29th</td>\n",
       "      <td>http://createsend.com/t/d-4A588D110F7B4D602540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Protecting Data in AI Health Systems</td>\n",
       "      <td>Wednesday, May 15th</td>\n",
       "      <td>http://createsend.com/t/d-4BDE76DC305BEB902540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Improving Health Care</td>\n",
       "      <td>Tuesday, March 5th</td>\n",
       "      <td>http://createsend.com/t/d-69D3E347E03D85602540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Health and Human Services Boosts Investment</td>\n",
       "      <td>Thursday, January 24th</td>\n",
       "      <td>http://createsend.com/t/d-EBF606F5AAA3AFAA2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Title                      Date  \\\n",
       "1                Linking EKG Data to Health Status    Wednesday, August 28th   \n",
       "22     Health Care, Sales Software Draw VC Funding        Tuesday, July 30th   \n",
       "104            Health Agency Launches AI Challenge         Monday, April 1st   \n",
       "126    Health Providers Develop AI Pathology Tools   Thursday, February 28th   \n",
       "127              Safeway Expands AI Health Clinics  Wednesday, February 27th   \n",
       "224  Using AI and Wearables to Improve Health Care       Wednesday, May 29th   \n",
       "391           Protecting Data in AI Health Systems       Wednesday, May 15th   \n",
       "440                          Improving Health Care        Tuesday, March 5th   \n",
       "466    Health and Human Services Boosts Investment    Thursday, January 24th   \n",
       "\n",
       "                                                           Link  \n",
       "1    http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED  \n",
       "22   http://createsend.com/t/d-A1BD7A3CB179A11F2540EF23F30FEDED  \n",
       "104  http://createsend.com/t/d-966FFE7CC9E33FD92540EF23F30FEDED  \n",
       "126  http://createsend.com/t/d-CB60BD202A582E9F2540EF23F30FEDED  \n",
       "127  http://createsend.com/t/d-F7543AEA82C99F6D2540EF23F30FEDED  \n",
       "224  http://createsend.com/t/d-4A588D110F7B4D602540EF23F30FEDED  \n",
       "391  http://createsend.com/t/d-4BDE76DC305BEB902540EF23F30FEDED  \n",
       "440  http://createsend.com/t/d-69D3E347E03D85602540EF23F30FEDED  \n",
       "466  http://createsend.com/t/d-EBF606F5AAA3AFAA2540EF23F30FEDED  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Health\n",
    "search_titles('Health')[['Title', 'Date', 'Link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Hackers Manipulate Machine Learning Data</td>\n",
       "      <td>Wednesday, February 13th</td>\n",
       "      <td>http://createsend.com/t/d-24F057A8365A95DB2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Twitter's Machine Learning</td>\n",
       "      <td>Monday, July 29th</td>\n",
       "      <td>http://createsend.com/t/d-D38F662D4FCCDFD12540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Learning More With Less Data</td>\n",
       "      <td>Wednesday, January 16th</td>\n",
       "      <td>http://createsend.com/t/d-C76FFC4D1B068C022540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Naspers Eyes Machine Learning</td>\n",
       "      <td>Monday, August 26th</td>\n",
       "      <td>http://createsend.com/t/d-5CCCECDBA202FB562540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Using Machine Learning to Boost Sales</td>\n",
       "      <td>Tuesday, May 14th</td>\n",
       "      <td>http://createsend.com/t/d-A817E226655F4F7A2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>Google Tightens Machine-Learning Data Security</td>\n",
       "      <td>Thursday, March 7th</td>\n",
       "      <td>http://createsend.com/t/d-137AB5D233E983B42540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Deep Learning Could Change How We Invent</td>\n",
       "      <td>Wednesday, February 20th</td>\n",
       "      <td>http://createsend.com/t/d-FA66E99EAF9571BE2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title                      Date  \\\n",
       "136        Hackers Manipulate Machine Learning Data  Wednesday, February 13th   \n",
       "183                      Twitter's Machine Learning         Monday, July 29th   \n",
       "314                    Learning More With Less Data   Wednesday, January 16th   \n",
       "322                   Naspers Eyes Machine Learning       Monday, August 26th   \n",
       "392           Using Machine Learning to Boost Sales         Tuesday, May 14th   \n",
       "438  Google Tightens Machine-Learning Data Security       Thursday, March 7th   \n",
       "449        Deep Learning Could Change How We Invent  Wednesday, February 20th   \n",
       "\n",
       "                                                           Link  \n",
       "136  http://createsend.com/t/d-24F057A8365A95DB2540EF23F30FEDED  \n",
       "183  http://createsend.com/t/d-D38F662D4FCCDFD12540EF23F30FEDED  \n",
       "314  http://createsend.com/t/d-C76FFC4D1B068C022540EF23F30FEDED  \n",
       "322  http://createsend.com/t/d-5CCCECDBA202FB562540EF23F30FEDED  \n",
       "392  http://createsend.com/t/d-A817E226655F4F7A2540EF23F30FEDED  \n",
       "438  http://createsend.com/t/d-137AB5D233E983B42540EF23F30FEDED  \n",
       "449  http://createsend.com/t/d-FA66E99EAF9571BE2540EF23F30FEDED  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning\n",
    "search_titles('Learning')[['Title', 'Date', 'Link']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Odd/Less Useful Features\n",
    "Likely candidates of additional stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Security Firm Uses AI to Deploy Resources</td>\n",
       "      <td>Tuesday, August 13th</td>\n",
       "      <td>http://createsend.com/t/d-2EE1F12EA5669D802540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NHL Uses Chatbot to Interact With Fans</td>\n",
       "      <td>Thursday, June 6th</td>\n",
       "      <td>http://createsend.com/t/d-588B2BD3B5B7DBD22540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>ICE Uses Facial Recognition on Driver's Photos</td>\n",
       "      <td>Tuesday, July 9th</td>\n",
       "      <td>http://createsend.com/t/d-53625F05CAA128452540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Walmart Uses AI to Monitor Store</td>\n",
       "      <td>Friday, April 26th</td>\n",
       "      <td>http://createsend.com/t/d-4E296F56B9D9ACFA2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>Unilever Uses AI to Cut Costs</td>\n",
       "      <td>Tuesday, July 16th</td>\n",
       "      <td>http://createsend.com/t/d-F19F782D41E4F2FF2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Amazon Uses AI Across Its Business</td>\n",
       "      <td>Thursday, June 6th</td>\n",
       "      <td>http://createsend.com/t/d-588B2BD3B5B7DBD22540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title                  Date  \\\n",
       "12        Security Firm Uses AI to Deploy Resources  Tuesday, August 13th   \n",
       "58           NHL Uses Chatbot to Interact With Fans    Thursday, June 6th   \n",
       "197  ICE Uses Facial Recognition on Driver's Photos     Tuesday, July 9th   \n",
       "246                Walmart Uses AI to Monitor Store    Friday, April 26th   \n",
       "350                   Unilever Uses AI to Cut Costs    Tuesday, July 16th   \n",
       "376              Amazon Uses AI Across Its Business    Thursday, June 6th   \n",
       "\n",
       "                                                           Link  \n",
       "12   http://createsend.com/t/d-2EE1F12EA5669D802540EF23F30FEDED  \n",
       "58   http://createsend.com/t/d-588B2BD3B5B7DBD22540EF23F30FEDED  \n",
       "197  http://createsend.com/t/d-53625F05CAA128452540EF23F30FEDED  \n",
       "246  http://createsend.com/t/d-4E296F56B9D9ACFA2540EF23F30FEDED  \n",
       "350  http://createsend.com/t/d-F19F782D41E4F2FF2540EF23F30FEDED  \n",
       "376  http://createsend.com/t/d-588B2BD3B5B7DBD22540EF23F30FEDED  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses\n",
    "search_titles('Uses')[['Title', 'Date', 'Link']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Push for AI Self-Sufficiency</td>\n",
       "      <td>Friday, May 10th</td>\n",
       "      <td>http://createsend.com/t/d-7EAB3F0986D964CA2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Automation May Push Women From Workforce</td>\n",
       "      <td>Wednesday, June 5th</td>\n",
       "      <td>http://createsend.com/t/d-01D69E144C2200182540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Google's New Robot Push</td>\n",
       "      <td>Thursday, March 28th</td>\n",
       "      <td>http://createsend.com/t/d-723D32C4C8C196D22540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>Stanford's Benevolent Machine Push</td>\n",
       "      <td>Tuesday, March 19th</td>\n",
       "      <td>http://createsend.com/t/d-E3AF4DB6ADB01D072540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Synchrony's AI Push</td>\n",
       "      <td>Wednesday, August 28th</td>\n",
       "      <td>http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Google's Latest Push</td>\n",
       "      <td>Tuesday, May 7th</td>\n",
       "      <td>http://createsend.com/t/d-229DE7069CD2AEEE2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>China's Research Push</td>\n",
       "      <td>Thursday, March 14th</td>\n",
       "      <td>http://createsend.com/t/d-37AC3C321823439D2540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Amazon's Autonomous Vehicle Push</td>\n",
       "      <td>Monday, February 25th</td>\n",
       "      <td>http://createsend.com/t/d-BB039B06392DA5272540EF23F30FEDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Title                    Date  \\\n",
       "76               Push for AI Self-Sufficiency        Friday, May 10th   \n",
       "219  Automation May Push Women From Workforce     Wednesday, June 5th   \n",
       "265                   Google's New Robot Push    Thursday, March 28th   \n",
       "272        Stanford's Benevolent Machine Push     Tuesday, March 19th   \n",
       "320                       Synchrony's AI Push  Wednesday, August 28th   \n",
       "397                      Google's Latest Push        Tuesday, May 7th   \n",
       "433                     China's Research Push    Thursday, March 14th   \n",
       "446          Amazon's Autonomous Vehicle Push   Monday, February 25th   \n",
       "\n",
       "                                                           Link  \n",
       "76   http://createsend.com/t/d-7EAB3F0986D964CA2540EF23F30FEDED  \n",
       "219  http://createsend.com/t/d-01D69E144C2200182540EF23F30FEDED  \n",
       "265  http://createsend.com/t/d-723D32C4C8C196D22540EF23F30FEDED  \n",
       "272  http://createsend.com/t/d-E3AF4DB6ADB01D072540EF23F30FEDED  \n",
       "320  http://createsend.com/t/d-A427D2A4C0EFD1F82540EF23F30FEDED  \n",
       "397  http://createsend.com/t/d-229DE7069CD2AEEE2540EF23F30FEDED  \n",
       "433  http://createsend.com/t/d-37AC3C321823439D2540EF23F30FEDED  \n",
       "446  http://createsend.com/t/d-BB039B06392DA5272540EF23F30FEDED  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push\n",
    "search_titles('Push')[['Title', 'Date', 'Link']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
